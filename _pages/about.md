---
permalink: /
# title: "About Me"
# excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<!-- [Bio](#bio)
====== -->
I am a senior research scientist and manager at Apple (Seattle). I lead an CV/ML applied research team, with a focus on neural rendering for AR/VR applications. I did my Ph.D. at <a href="https://www.mit.edu">MIT</a> and my undergrad at <a href="https://hkust.edu.hk/">HKUST</a>.

Our team at Apple is actively hiring both research interns and fulltime research scientists. If you have a strong technical background and publication record in CV/ML/graphics, please feel free to reach out. 

[Research](#research)
======
I'm broadly interested in computer vision, machine learning, and computer graphics. Much of my research is about neural rendering, 3D reconstruction, and generative models.

<table>
  <div style="width: 25%; float: left"> 
    <!-- <a href="https://youtu.be/HbsUU0YODqE"> -->
      <img src="images/2022-eccv-gmpi.png" style="max-width: 90%"/>
    <!-- </a> -->
  </div>
  <div style="margin-left: 25%; vertical-align: middle"> 
    <a href="">Generative Multiplane Images: Making a 2D GAN 3D-Aware</a>
    <br>Xiaoming Zhao, ‪<u>Fangchang Ma</u>, David Güera, Zhile Ren, Alexander Schwing, Alex Colburn
    <br> ECCV 2022 <font color="red"><strong>(Oral Presentation)</strong></font>
  </div>
</table>

<table>
  <div style="width: 25%; float: left"> 
    <img src="images/2022-eccv-texturify.png" style="max-width: 90%"/>
  </div>
  <div style="margin-left: 25%"> 
    <a href="https://arxiv.org/abs/2204.02411">Texturify: Generating Textures on 3D Shape Surfaces</a>
    <br>Yawar Siddiqui, Justus Thies, ‪<u>Fangchang Ma</u>, ‪Qi Shan, Matthias Nießner, Angela Dai
    <br> ECCV 2022
      <a href="https://arxiv.org/abs/2204.02411">[pdf]</a>
      <!-- <a href="https://github.com/fangchangma/self-supervised-depth-completion">[code]</a> -->
      <a href="https://youtu.be/M5OU_fiD3Jk">[video]</a>
      <a href="https://nihalsid.github.io/texturify/">[webpage]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left"> 
    <img src="images/2021-iccv-retrievalfuse.jpg" style="max-width: 90%"/>
  </div>
  <div style="margin-left: 25%"> 
    <a href="https://arxiv.org/abs/2104.00024">RetrievalFuse: Neural 3D Scene Reconstruction with a Database</a>
    <br>Yawar Siddiqui, Justus Thies, ‪<u>Fangchang Ma</u>, ‪Qi Shan, Matthias Nießner, Angela Dai
    <br> ICCV 2021
      <a href="https://arxiv.org/abs/2104.00024">[pdf]</a>
      <a href="https://github.com/nihalsid/retrieval-fuse">[code]</a>
      <a href="https://youtu.be/HbsUU0YODqE">[video]</a>
      <a href="https://nihalsid.github.io/retrieval-fuse/">[webpage]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left"> 
    <img src="images/2019-icra-self-supervised.gif" style="max-width: 90%"/>
  </div>
  <div style="margin-left: 25%"> 
    <a href="https://arxiv.org/pdf/1807.00275.pdf">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a>
    <br><u>Fangchang Ma</u>, Guilherme Venturelli Cavalheiro, Sertac Karaman
    <br> ICRA 2019
      <a href="https://arxiv.org/pdf/1807.00275.pdf">[pdf]</a>
      <a href="https://github.com/fangchangma/self-supervised-depth-completion">[code]</a>
      <a href="https://youtu.be/bGXfvF261pc">[video]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2019-icra-fastdepth.png" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="http://fastdepth.mit.edu/2019_icra_fastdepth.pdf">FastDepth: Fast Monocular Depth Estimation on Embedded Systems</a>
    <br>Diana Wofk*, <u>Fangchang Ma*</u>, Tien-Ju Yang, Sertac Karaman, Vivienne Sze
    <br> ICRA 2019
      <a href="http://fastdepth.mit.edu/2019_icra_fastdepth.pdf">[pdf]</a>
      <a href="https://github.com/dwofk/fast-depth">[code]</a>
      <a href="https://youtu.be/gRqrYJWyXyI">[video]</a>
      <a href="http://fastdepth.mit.edu/">[webpage]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2018-neurips-invertibility.png" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf">Invertibility of Convolutional Generative Networks from Partial Measurements</a>
    <br><u>Fangchang Ma</u>, Ulas Ayaz, Sertac Karaman
    <br> NeurIPS 2018
    <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf">[pdf]</a>
    <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements-supplemental.zip">[supplementary]</a>
    <a href="https://github.com/fangchangma/invert-generative-networks">[code]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2018-icra-sparse-to-dense.gif" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="https://arxiv.org/pdf/1709.07492.pdf">Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image</a>
    <br><u>Fangchang Ma</u>, Sertac Karaman
    <br> ICRA 2018
    <a href="https://arxiv.org/pdf/1709.07492.pdf">[pdf]</a>
    <a href="https://youtu.be/vNIIT_M7x7Y">[video]</a>
    <a href="https://github.com/fangchangma/sparse-to-dense.pytorch">[pytorch code]</a>
    <a href="https://github.com/fangchangma/sparse-to-dense">[torch code]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2016-iros-sparse-depth-sensing.gif" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="https://arxiv.org/pdf/1703.01398.pdf">Sparse Depth Sensing for Resource-Constrained Robots</a>
    <br><u>Fangchang Ma</u>, Luca Carlone, Ulas Ayaz, Sertac Karaman
    <br> IROS 2016 & IJRR
    <a href="https://arxiv.org/pdf/1703.01398.pdf">[pdf]</a>
    <a href="https://github.com/sparse-depth-sensing/sparse-depth-sensing">[code]</a>
    <a href="https://youtu.be/vE56akCGeJQ">[video]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2014-wafr.png" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="https://arxiv.org/pdf/1704.02075.pdf">On Sensing, Agility, and Computation Requirements for a Data-gathering Vehicle</a>
    <br><u>Fangchang Ma</u>, Sertac Karaman
    <br> WAFR 2014
    <a href="https://arxiv.org/pdf/1704.02075.pdf">[pdf]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2013-robio.jpg" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <a href="https://www.researchgate.net/profile/Fangchang_Ma/publication/271548386_Velocity_estimator_via_fusing_inertial_measurements_and_multiple_feature_correspondences_from_a_single_camera/links/58be525ba6fdcc2d14eb5afd/Velocity-estimator-via-fusing-inertial-measurements-and-multiple-feature-correspondences-from-a-single-camera.pdf">Velocity estimator via fusing inertial measurements and multiple feature correspondences from a single camera</a>
    <br>Guyue Zhou, <u>Fangchang Ma</u>, Zexiang Li, Tao Wang
    <br> ROBIO 2013
    <a href="https://www.researchgate.net/profile/Fangchang_Ma/publication/271548386_Velocity_estimator_via_fusing_inertial_measurements_and_multiple_feature_correspondences_from_a_single_camera/links/58be525ba6fdcc2d14eb5afd/Velocity-estimator-via-fusing-inertial-measurements-and-multiple-feature-correspondences-from-a-single-camera.pdf">[pdf]</a>
  </div>
</table>
