---
permalink: /
# title: "Fangchang Ma"
# excerpt: "About me"
author_profile: true
# redirect_from:
#   - /about/
#   - /about.html
---

<h2>About Me</h2>
I was a research scientist at Apple, leading an applied research team with a focus on neural rendering and generative AI. I did my Ph.D. at <a href="https://www.mit.edu">MIT</a> and my undergrad at <a href="https://hkust.edu.hk/">HKUST</a>.

<h2>Shipped Products</h2>
<p style="line-height: 1.0;">A number of products that I contributed to:</p>
<ul style="line-height: 1.0;">
    <li>Depth estimation on ARKit</li>
    <li>Depth estimation on Vision Pro</li>
    <li>Face reconstruction for Persona (3D Facetime on Vision Pro)</li>
    <li>24MP imaging on iPhone Pro</li>
</ul>

<h2>Research Highlights</h2>
I'm broadly interested in computer vision, machine learning, and computer graphics. Much of my research is about neural rendering, 3D reconstruction, and generative models. A complete list of my publication can be found on my <a href="https://scholar.google.com/citations?hl=en&user=kf07AjoAAAAJ&view_op=list_works" target="_blank">Google Scholar</a>.

<table style="border-collapse: collapse; border: none;">
  <!-- <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2023-stabledreamer.jpg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/abs/2312.02189">StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D</a>
      <br>Pengsheng Guo, Hans Hao, Adam Caccavale, Alexander G. Schwing, Zhongzheng Ren, Alex Colburn, Edward Zhang, <u>Fangchang Ma</u>
      <br> 
      <a href="https://arxiv.org/abs/2312.02189">[arXiv]</a>
    </td>
  </tr> -->

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2023-hyperdiffusion.png" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://ziyaerkoc.com/hyperdiffusion/">HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion</a>
      <br>Ziya Erkoç, <u>Fangchang Ma</u>, Qi Shan, Matthias Nießner, Angela Dai
      <br> ICCV, 2023
      <br> 
      <a href="https://arxiv.org/abs/2303.17015">[arXiv]</a>
      <a href="https://ziyaerkoc.com/hyperdiffusion/">[webpage]</a>
    </td>
  </tr>

  <!-- <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2023-finerecon.jpeg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/abs/2304.01480">
      FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction
      </a>
      <br>Noah Stier, Anurag Ranjan, Alex Colburn, Yajie Yan, Liang Yang, <u>Fangchang Ma</u>, Baptiste Angles
      <br> ICCV, 2023
      <br> 
      <a href="https://arxiv.org/abs/2304.01480">[arXiv]</a>
    </td>
  </tr> -->

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2022-eccv-gmpi.jpg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://xiaoming-zhao.github.io/projects/gmpi/">Generative Multiplane Images: Making a 2D GAN 3D-Aware</a>
      <br>Xiaoming Zhao, <u>Fangchang Ma</u>, David Güera, Zhile Ren, Alexander Schwing, Alex Colburn
      <br> ECCV, 2022 <font color="red"><strong>(Oral Presentation)</strong></font>
      <br> 
      <a href="https://arxiv.org/abs/2207.10642">[arXiv]</a>
      <a href="https://github.com/apple/ml-gmpi">[code]</a>
      <!-- <a href="https://youtu.be/M5OU_fiD3Jk">[video]</a> -->
      <a href="https://xiaoming-zhao.github.io/projects/gmpi/">[webpage]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2022-eccv-texturify.jpg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://nihalsid.github.io/texturify/">Texturify: Generating Textures on 3D Shape Surfaces</a>
      <br>Yawar Siddiqui, Justus Thies, <u>Fangchang Ma</u>, Qi Shan, Matthias Nießner, Angela Dai
      <br> ECCV, 2022
      <br>
      <a href="https://arxiv.org/abs/2204.02411">[arXiv]</a>
      <!-- <a href="https://github.com/fangchangma/self-supervised-depth-completion">[code]</a> -->
      <a href="https://youtu.be/M5OU_fiD3Jk">[video]</a>
      <a href="https://nihalsid.github.io/texturify/">[webpage]</a>
    </td>
  </tr>

  <!-- <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2021-iccv-retrievalfuse.jpg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://nihalsid.github.io/retrieval-fuse/">RetrievalFuse: Neural 3D Scene Reconstruction with a Database</a>
      <br>Yawar Siddiqui, Justus Thies, <u>Fangchang Ma</u>, Qi Shan, Matthias Nießner, Angela Dai
      <br> ICCV, 2021
      <br>
      <a href="https://arxiv.org/abs/2104.00024">[arXiv]</a>
      <a href="https://github.com/nihalsid/retrieval-fuse">[code]</a>
      <a href="https://youtu.be/HbsUU0YODqE">[video]</a>
      <a href="https://nihalsid.github.io/retrieval-fuse/">[webpage]</a>
    </td>
  </tr> -->

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2019-icra-self-supervised.gif" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/pdf/1807.00275.pdf">Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</a>
      <br><u>Fangchang Ma</u>, Guilherme Venturelli Cavalheiro, Sertac Karaman
      <br> ICRA, 2019
      <br>
      <a href="https://arxiv.org/pdf/1807.00275.pdf">[arXiv]</a>
      <a href="https://github.com/fangchangma/self-supervised-depth-completion">[code]</a>
      <a href="https://youtu.be/bGXfvF261pc">[video]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2019-icra-fastdepth.png" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="http://fastdepth.mit.edu/">FastDepth: Fast Monocular Depth Estimation on Embedded Systems</a>
      <br>Diana Wofk*, <u>Fangchang Ma*</u>, Tien-Ju Yang, Sertac Karaman, Vivienne Sze
      <br> ICRA, 2019
      <br>
      <a href="https://arxiv.org/abs/1903.03273">[arXiv]</a>
      <a href="https://github.com/dwofk/fast-depth">[code]</a>
      <a href="https://youtu.be/gRqrYJWyXyI">[video]</a>
      <a href="http://fastdepth.mit.edu/">[webpage]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2018-neurips-invertibility.png" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf">Invertibility of Convolutional Generative Networks from Partial Measurements</a>
      <br><u>Fangchang Ma</u>, Ulas Ayaz, Sertac Karaman
      <br> NeurIPS, 2018
      <br>
      <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf">[pdf]</a>
      <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements-supplemental.zip">[supplementary]</a>
      <a href="https://github.com/fangchangma/invert-generative-networks">[code]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2018-icra-sparse-to-dense.gif" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/abs/1709.07492">Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image</a>
      <br><u>Fangchang Ma</u>, Sertac Karaman
      <br> ICRA, 2018
      <br>
      <a href="https://arxiv.org/abs/1709.07492">[arXiv]</a>
      <a href="https://youtu.be/vNIIT_M7x7Y">[video]</a>
      <a href="https://github.com/fangchangma/sparse-to-dense.pytorch">[pytorch code]</a>
      <a href="https://github.com/fangchangma/sparse-to-dense">[torch code]</a>
    </td>
  </tr>

  <!-- <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2016-iros-sparse-depth-sensing.gif" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/abs/1703.01398">Sparse Depth Sensing for Resource-Constrained Robots</a>
      <br><u>Fangchang Ma</u>, Luca Carlone, Ulas Ayaz, Sertac Karaman
      <br> IROS, 2016
      <br> IJRR (extended version)
      <br>
      <a href="https://arxiv.org/abs/1703.01398">[arXiv]</a>
      <a href="https://github.com/sparse-depth-sensing/sparse-depth-sensing">[code]</a>
      <a href="https://youtu.be/vE56akCGeJQ">[video]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2014-wafr.png" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://arxiv.org/abs/1704.02075">On Sensing, Agility, and Computation Requirements for a Data-gathering Vehicle</a>
      <br><u>Fangchang Ma</u>, Sertac Karaman
      <br> WAFR, 2014
      <br>
      <a href="https://arxiv.org/abs/1704.02075">[arXiv]</a>
    </td>
  </tr>

  <tr style="border: none;">
    <td style="align-items:center; width: 25%; border: none;">
      <img src="images/2013-robio.jpg" style=" vertical-align:middle"/>
    </td>
    <td style="align-items:center; border: none;">
      <a href="https://www.researchgate.net/profile/Fangchang_Ma/publication/271548386_Velocity_estimator_via_fusing_inertial_measurements_and_multiple_feature_correspondences_from_a_single_camera/links/58be525ba6fdcc2d14eb5afd/Velocity-estimator-via-fusing-inertial-measurements-and-multiple-feature-correspondences-from-a-single-camera.pdf">Velocity estimator via fusing inertial measurements and multiple feature correspondences from a single camera</a>
      <br>Guyue Zhou, <u>Fangchang Ma</u>, Zexiang Li, Tao Wang
      <br> ROBIO, 2013
      <br>
      <a href="https://www.researchgate.net/profile/Fangchang_Ma/publication/271548386_Velocity_estimator_via_fusing_inertial_measurements_and_multiple_feature_correspondences_from_a_single_camera/links/58be525ba6fdcc2d14eb5afd/Velocity-estimator-via-fusing-inertial-measurements-and-multiple-feature-correspondences-from-a-single-camera.pdf">[pdf]</a>
    </td>
  </tr> -->

</table>
