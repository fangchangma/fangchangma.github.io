---
permalink: /
# title: "About Me"
# excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research scientist at Apple (Seattle). I am broadly interested in computer vision, machine learning, and applications in AR/VR. Some of the research topics I've worked on include neural rendering, depth estimation, generative neural networks, and self-supervised learning.

If you have a strong background in computer vision / machine learning / computer graphics and are looking for exciting opportunities (either internship or full-time) at Apple, feel free to reach out.


[Bio](#bio)
======
I finished my Ph.D. in Autonomous Systems at Massachusetts Institute of Technolgoy in 2019, advised by [Prof. Sertac Karaman](http://sertac.scripts.mit.edu/web/). My thesis was on *Algorithms for Single-View Depth Image Estimation*. Prior to that, I obtained my bachelor's degree in Computer Engineering from Hong Kong University of Science and Technology in 2013.


[Publications](#publications)
======
<table>
  <div style="width: 25%; float: left"> 
    <a href="https://youtu.be/bGXfvF261pc">
      <img src="images/2019-icra-self-supervised.gif" style="max-width: 90%"/>
    </a>
  </div>
  <div style="margin-left: 25%"> 
    <b>Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from LiDAR and Monocular Camera</b>
    <br><u>Fangchang Ma</u>, Guilherme Venturelli Cavalheiro, Sertac Karaman
    <br> ICRA 2019
      <a href="https://arxiv.org/pdf/1807.00275.pdf">[pdf]</a>
      <a href="https://github.com/fangchangma/self-supervised-depth-completion">[code]</a>
      <a href="https://youtu.be/bGXfvF261pc">[video]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <a href="https://youtu.be/gRqrYJWyXyI">
      <img src="images/2019-icra-fastdepth.png" style="max-width: 90%;"/>
    </a>
  </div>
  <div style="margin-left: 25%;"> 
    <b>FastDepth: Fast Monocular Depth Estimation on Embedded Systems</b>
    <br>Diana Wofk*, <u>Fangchang Ma*</u>, Tien-Ju Yang, Sertac Karaman, Vivienne Sze
    <br> ICRA 2019
      <a href="http://fastdepth.mit.edu/2019_icra_fastdepth.pdf">[pdf]</a>
      <a href="https://github.com/dwofk/fast-depth">[code]</a>
      <a href="http://fastdepth.mit.edu/">[webpage]</a>
      <a href="https://youtu.be/gRqrYJWyXyI">[video]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2018-neurips-invertibility.png" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <b>Invertibility of Convolutional Generative Networks from Partial Measurements</b>
    <br><u>Fangchang Ma</u>, Ulas Ayaz, Sertac Karaman
    <br> NeurIPS 2018
    <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements.pdf">[pdf]</a>
    <a href="https://papers.nips.cc/paper/8171-invertibility-of-convolutional-generative-networks-from-partial-measurements-supplemental.zip">[supplementary]</a>
    <a href="https://github.com/fangchangma/invert-generative-networks">[code]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <a href="https://youtu.be/vNIIT_M7x7Y"> 
      <img src="images/2018-icra-sparse-to-dense.gif" style="max-width: 90%;"/>
    </a>
  </div>
  <div style="margin-left: 25%;"> 
    <b>Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image</b>
    <br><u>Fangchang Ma</u>, Sertac Karaman
    <br> ICRA 2018
    <a href="https://arxiv.org/pdf/1709.07492.pdf">[pdf]</a>
    <a href="https://youtu.be/vNIIT_M7x7Y">[video]</a>
    <a href="https://github.com/fangchangma/sparse-to-dense.pytorch">[pytorch code]</a>
    <a href="https://github.com/fangchangma/sparse-to-dense">[torch code]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <a href="https://youtu.be/vE56akCGeJQ">
      <img src="images/2016-iros-sparse-depth-sensing.gif" style="max-width: 90%;"/>
    </a>
  </div>
  <div style="margin-left: 25%;"> 
    <b>Sparse Depth Sensing for Resource-Constrained Robots</b>
    <br><u>Fangchang Ma</u>, Luca Carlone, Ulas Ayaz, Sertac Karaman
    <br> IROS 2016 & IJRR
    <a href="https://arxiv.org/pdf/1703.01398.pdf">[pdf]</a>
    <a href="https://github.com/sparse-depth-sensing/sparse-depth-sensing">[code]</a>
    <a href="https://youtu.be/vE56akCGeJQ">[video]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2014-wafr.png" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <b>On Sensing, Agility, and Computation Requirements for a Data-gathering Vehicle</b>
    <br><u>Fangchang Ma</u>, Sertac Karaman
    <br> WAFR 2014
    <a href="https://arxiv.org/pdf/1704.02075.pdf">[pdf]</a>
  </div>
</table>

<table>
  <div style="width: 25%; float: left;"> 
    <img src="images/2013-robio.jpg" style="max-width: 90%;"/>
  </div>
  <div style="margin-left: 25%;"> 
    <b>Velocity estimator via fusing inertial measurements and multiple feature correspondences from a single camera</b>
    <br>Guyue Zhou, <u>Fangchang Ma</u>, Zexiang Li, Tao Wang
    <br> ROBIO 2013
    <a href="https://www.researchgate.net/profile/Fangchang_Ma/publication/271548386_Velocity_estimator_via_fusing_inertial_measurements_and_multiple_feature_correspondences_from_a_single_camera/links/58be525ba6fdcc2d14eb5afd/Velocity-estimator-via-fusing-inertial-measurements-and-multiple-feature-correspondences-from-a-single-camera.pdf">[pdf]</a>
  </div>
</table>

[Service](#service)
======
**Conference Reviewing**: [NeurIPS](https://nips.cc/), [ICRA](http://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra), [IROS](http://www.iros.org/) <br/> 
**Journal Reviewing**: [PAMI](https://www.computer.org/csdl/journal/tp), [Pattern Recognition Letters](https://www.journals.elsevier.com/pattern-recognition-letters), [IJRR](https://journals.sagepub.com/home/ijr), [RA-L](http://www.ieee-ras.org/publications/ra-l), [RAM](http://www.ieee-ras.org/publications/ram), [IEEE Sensors Journal](https://ieee-sensors.org/sensors-journal/), [Unmanned Systems](https://www.worldscientific.com/worldscinet/us) <br/> 